<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>Machine Learning for Time Series</title>
  <meta name="description" content="# Shapelet-based Representations and Convolutional ModelsIn this section, we will cover works that either relate to the Shapeletrepresentation for time serie...">

  <link rel="canonical" href="https://rtavenar.github.io/hdr/content/parts/02/shapelets_cnn.html">
  <link rel="alternate" type="application/rss+xml" title="Machine Learning for Time Series" href="https://rtavenar.github.io/hdr/feed.xml">

  <meta property="og:url"         content="https://rtavenar.github.io/hdr/content/parts/02/shapelets_cnn.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Machine Learning for Time Series" />
<meta property="og:description" content="# Shapelet-based Representations and Convolutional ModelsIn this section, we will cover works that either relate to the Shapeletrepresentation for time serie..." />
<meta property="og:image"       content="https://rtavenar.github.io/hdr/false" />

<meta name="twitter:card" content="summary">


  <script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage": "https://rtavenar.github.io/hdr/content/parts/02/shapelets_cnn.html",
  "headline": "Machine Learning for Time Series",
  "datePublished": "2020-05-04T18:58:08+02:00",
  "dateModified": "2020-05-04T18:58:08+02:00",
  "description": "# Shapelet-based Representations and Convolutional ModelsIn this section, we will cover works that either relate to the Shapeletrepresentation for time serie...",
  "author": {
    "@type": "Person",
    "name": "Romain Tavenard"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data 100 at UC Berkeley",
    "logo": {
      "@type": "ImageObject",
      "url": "https://rtavenar.github.io/hdr",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "https://rtavenar.github.io/hdr",
    "height": 60,
    "width": 60
  }
}

  </script>
  <link rel="stylesheet" href="/hdr/assets/css/styles.css">

  <!-- <link rel="manifest" href="/manifest.json"> -->
  <!-- <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#efae0a"> -->
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#233947">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="/hdr/images/logo/favicon.ico">

  <!-- MathJax Config -->
  <!-- Allow inline math using $ and automatically break long math lines -->
<!-- (mostly) copied from nbconvert configuration -->
<!-- https://github.com/jupyter/nbconvert/blob/master/nbconvert/templates/html/mathjax.tpl -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}},
        linebreaks: { automatic: true },
    },
    
    // Number LaTeX-style equations
    "TeX": {
        equationNumbers: {
          autoNumber: "all"
        }
    }
    
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML' async></script>


  <!-- DOM updating function -->
  <script src="/hdr/assets/js/page/dom-update.js"></script>

  <!-- Selectors for elements on the page -->
  <script src="/hdr/assets/js/page/documentSelectors.js"></script>

  <!-- Define some javascript variables that will be useful in other javascript -->
  <script>
    const site_basename = '/hdr';
  </script>

  <!-- Add AnchorJS to let headers be linked -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.min.js" async></script>
  <script src="/hdr/assets/js/page/anchors.js" async></script>

  <!-- Include Turbolinks to make page loads fast -->
  <!-- https://github.com/turbolinks/turbolinks -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/turbolinks/5.2.0/turbolinks.js" async></script>
  <meta name="turbolinks-cache-control" content="no-cache">

  <!-- Load nbinteract for widgets -->
  

  <!-- Load Thebelab for interactive widgets -->
  <!-- Include Thebelab for interactive code if it's enabled -->


<!-- Display Thebelab button in each code cell -->
<script>
/**
 * Set up thebelab button for code blocks
 */

const thebelabCellButton = id =>
  `<a id="thebelab-cell-button-${id}" class="btn thebebtn o-tooltip--left" data-tooltip="Interactive Mode">
    <img src="/hdr/assets/images/edit-button.svg" alt="Start thebelab interactive mode">
  </a>`


const addThebelabButtonToCodeCells =  () => {

  const codeCells = document.querySelectorAll('div.input_area > div.highlight:not(.output) pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.getElementById("thebelab-cell-button-" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', thebelabCellButton(id));
    }
  })
}

initFunction(addThebelabButtonToCodeCells);
</script>


<script src="https://unpkg.com/thebelab@latest/lib/index.js" async></script>
<script>
    /**
     * Add attributes to Thebelab blocks
     */

    const initThebelab = () => {
        const addThebelabToCodeCells = () => {
            console.log("Adding thebelab to code cells...");
            // If Thebelab hasn't loaded, wait a bit and try again. This
            // happens because we load ClipboardJS asynchronously.
            if (window.thebelab === undefined) {
                setTimeout(addThebelabToCodeCells, 250)
            return
            }

            // If we already detect a Thebelab cell, don't re-run
            if (document.querySelectorAll('div.thebelab-cell').length > 0) {
                return;
            }

            // Find all code cells, replace with Thebelab interactive code cells
            const codeCells = document.querySelectorAll('.input_area pre')
            codeCells.forEach((codeCell, index) => {
                const id = codeCellId(index)

                // Clean up the language to make it work w/ CodeMirror and add it to the cell
                dataLanguage = ""
                dataLanguage = detectLanguage(dataLanguage);
                codeCell.setAttribute('data-language', dataLanguage)
                codeCell.setAttribute('data-executable', 'true')

                // If the code cell is hidden, show it
                var inputCheckbox = document.querySelector(`input#hidebtn${codeCell.id}`);
                if (inputCheckbox !== null) {
                    setCodeCellVisibility(inputCheckbox, 'visible');
                }
            });

            // Remove the event listener from the page so keyboard press doesn't
            // Change page
            document.removeEventListener('keydown', initPageNav)
            keyboardListener = false;

            // Init thebelab
            thebelab.bootstrap();

            // Remove copy buttons since they won't work anymore
            const copyAndThebeButtons = document.querySelectorAll('.copybtn, .thebebtn')
            copyAndThebeButtons.forEach((button, index) => {
                button.remove();
            });

            // Remove outputs since they'll be stale
            const outputs = document.querySelectorAll('.output *, .output')
            outputs.forEach((output, index) => {
                output.remove();
            });

            // Find any cells with an initialization tag and ask ThebeLab to run them when ready
            var thebeInitCells = document.querySelectorAll('div.tag_thebelab-init');
            thebeInitCells.forEach((cell) => {
                console.log("Initializing ThebeLab with cell: " + cell.id);
                cell.querySelector('.thebelab-run-button').click();
            });
        }

        // Add event listener for the function to modify code cells
        const thebelabButtons = document.querySelectorAll('[id^=thebelab], [id$=thebelab]')
        thebelabButtons.forEach((thebelabButton,index) => {
            if (thebelabButton === null) {
                setTimeout(initThebelab, 250)
                return
            };
            thebelabButton.addEventListener('click', addThebelabToCodeCells);
        });
    }

    // Initialize Thebelab
    initFunction(initThebelab);

// Helper function to munge the language name
var detectLanguage = (language) => {
    if (language.indexOf('python') > -1) {
        language = "python";
    }
    return language;
}
</script>



  <!-- Load the auto-generating TOC (non-async otherwise the TOC won't load w/ turbolinks) -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.8.1/tocbot.min.js" async></script>
  <script src="/hdr/assets/js/page/tocbot.js"></script>

  <!-- Google analytics -->
  


  <!-- Clipboard copy button -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script>

  <!-- Load custom website scripts -->
  <script src="/hdr/assets/js/scripts.js" async></script>

  <!-- Load custom user CSS and JS  -->
  <script src="/hdr/assets/custom/custom.js" async></script>
  <link rel="stylesheet" href="/hdr/assets/custom/custom.css">

  <!-- Update interact links w/ REST param, is defined in includes so we can use templates -->
  
<script>
/**
  * To auto-embed hub URLs in interact links if given in a RESTful fashion
 */

function getJsonFromUrl(url) {
  var query = url.split('?');
  if (query.length < 2) {
    // No queries so just return false
    return false;
  }
  query = query[1];
  // Collect REST params into a dictionary
  var result = {};
  query.split("&").forEach(function(part) {
    var item = part.split("=");
    result[item[0]] = decodeURIComponent(item[1]);
  });
  return result;
}
    
function dict2param(dict) {
    params = Object.keys(dict).map(function(k) {
        return encodeURIComponent(k) + '=' + encodeURIComponent(dict[k])
    });
    return params.join('&')
}

// Parse a Binder URL, converting it to the string needed for JupyterHub
function binder2Jupyterhub(url) {
  newUrl = {};
  parts = url.split('v2/gh/')[1];
  // Grab the base repo information
  repoinfo = parts.split('?')[0];
  var [org, repo, ref] = repoinfo.split('/');
  newUrl['repo'] = ['https://github.com', org, repo].join('/');
  newUrl['branch'] = ref
  // Grab extra parameters passed
  params = getJsonFromUrl(url);
  if (params['filepath'] !== undefined) {
    newUrl['subPath'] = params['filepath']
  }
  return dict2param(newUrl);
}

// Filter out potentially unsafe characters to prevent xss
function safeUrl(url)
{
   return String(encodeURIComponent(url))
            .replace(/&/g, '&amp;')
            .replace(/"/g, '&quot;')
            .replace(/'/g, '&#39;')
            .replace(/</g, '&lt;')
            .replace(/>/g, '&gt;');
}

function addParamToInternalLinks(hub) {
  var links = document.querySelectorAll("a").forEach(function(link) {
    var href = link.href;
    // If the link is an internal link...
    if (href.search("https://rtavenar.github.io") !== -1 || href.startsWith('/') || href.search("127.0.0.1:") !== -1) {
      // Assume we're an internal link, add the hub param to it
      var params = getJsonFromUrl(href);
      if (params !== false) {
        // We have REST params, so append a new one
        params['jupyterhub'] = hub;
      } else {
        // Create the REST params
        params = {'jupyterhub': hub};
      }
      // Update the link
      var newHref = href.split('?')[0] + '?' + dict2param(params);
      link.setAttribute('href', decodeURIComponent(newHref));
    }
  });
  return false;
}


// Update interact links
function updateInteractLink() {
    // hack to make this work since it expects a ? in the URL
    rest = getJsonFromUrl("?" + location.search.substr(1));
    jupyterHubUrl = rest['jupyterhub'];
    var hubType = null;
    var hubUrl = null;
    if (jupyterHubUrl !== undefined) {
      hubType = 'jupyterhub';
      hubUrl = jupyterHubUrl;
    }

    if (hubType !== null) {
      // Sanitize the hubUrl
      hubUrl = safeUrl(hubUrl);

      // Add HTTP text if omitted
      if (hubUrl.indexOf('http') < 0) {hubUrl = 'http://' + hubUrl;}
      var interactButtons = document.querySelectorAll("button.interact-button")
      var lastButton = interactButtons[interactButtons.length-1];
      var link = lastButton.parentElement;

      // If we've already run this, skip the link updating
      if (link.nextElementSibling !== null) {
        return;
      }

      // Update the link and add context div
      var href = link.getAttribute('href');
      if (lastButton.id === 'interact-button-binder') {
        // If binder links exist, we need to re-work them for jupyterhub
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // If localhost, assume we're working from a local Jupyter server and remove `/hub`
          first = [hubUrl, 'git-sync'].join('/')
        } else {
          first = [hubUrl, 'hub', 'user-redirect', 'git-sync'].join('/')
        }
        href = first + '?' + binder2Jupyterhub(href);
      } else {
        // If interact button isn't binderhub, assume it's jupyterhub
        // If JupyterHub links, we only need to replace the hub url
        href = href.replace("", hubUrl);
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // Assume we're working from a local Jupyter server and remove `/hub`
          href = href.replace("/hub/user-redirect", "");
        }
      }
      link.setAttribute('href', decodeURIComponent(href));

      // Add text after interact link saying where we're launching
      hubUrlNoHttp = decodeURIComponent(hubUrl).replace('http://', '').replace('https://', '');
      link.insertAdjacentHTML('afterend', '<div class="interact-context">on ' + hubUrlNoHttp + '</div>');

      // Update internal links so we retain the hub url
      addParamToInternalLinks(hubUrl);
    }
}

runWhenDOMLoaded(updateInteractLink)
document.addEventListener('turbolinks:load', updateInteractLink)
</script>


  <!-- Lunr search code - will only be executed on the /search page -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.6/lunr.min.js" async></script>
  <script>var initQuery = function() {
  // See if we have a search box
  var searchInput = document.querySelector('input#lunr_search');
  if (searchInput === null) {
    return;
  }

  // Function to parse our lunr cache
  var idx = lunr(function () {
    this.field('title')
    this.field('excerpt')
    this.field('categories')
    this.field('tags')
    this.ref('id')

    this.pipeline.remove(lunr.trimmer)

    for (var item in store) {
      this.add({
        title: store[item].title,
        excerpt: store[item].excerpt,
        categories: store[item].categories,
        tags: store[item].tags,
        id: item
      })
    }
  });

  // Run search upon keyup
  searchInput.addEventListener('keyup', function () {
    var resultdiv = document.querySelector('#results');
    var query = document.querySelector("input#lunr_search").value.toLowerCase();
    var result =
      idx.query(function (q) {
        query.split(lunr.tokenizer.separator).forEach(function (term) {
          q.term(term, { boost: 100 })
          if(query.lastIndexOf(" ") != query.length-1){
            q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })
          }
          if (term != ""){
            q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })
          }
        })
      });

      // Empty the results div
      while (resultdiv.firstChild) {
        resultdiv.removeChild(resultdiv.firstChild);
      }

    resultdiv.insertAdjacentHTML('afterbegin', '<p class="results__found">'+result.length+' Result(s) found</p>');
    for (var item in result) {
      var ref = result[item].ref;
      if(store[ref].teaser){
        var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<div class="archive__item-teaser">'+
                '<img src="'+store[ref].teaser+'" alt="">'+
              '</div>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      else{
    	  var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      resultdiv.insertAdjacentHTML('beforeend', searchitem);
    }
  });
};

initFunction(initQuery);
</script>

  <!-- Load JS that depends on site variables -->
  <script src="/hdr/assets/js/page/copy-button.js" async></script>

  <!-- Hide cell code -->
  <script src="/hdr/assets/js/page/hide-cell.js" async></script>

  <!-- Printing the screen -->
  <!-- Include nbinteract for interactive widgets -->
<script src="https://printjs-4de6.kxcdn.com/print.min.js" async></script>
<script>
printContent = () => {
    // MathJax displays a second version of any math for assistive devices etc.
    // This prevents double-rendering in the PDF output.
    var ignoreAssistList = [];
    assistives = document.querySelectorAll('.MathJax_Display span.MJX_Assistive_MathML').forEach((element, index) => {
        var thisId = 'MathJax-assistive-' + index.toString();
        element.setAttribute('id', thisId);
        ignoreAssistList.push(thisId)
    });

    // Print the actual content object
    printJS({
        printable: 'textbook_content',
        type: 'html',
        css: "/hdr/assets/css/styles.css",
        style: "#textbook_content {padding-top: 40px};",
        scanStyles: false,
        targetStyles: ["*"],
        ignoreElements: ignoreAssistList,
        documentTitle: "Made with Jupyter Book"
    })
};

initPrint = () => {
    document.querySelector('#interact-button-print').addEventListener('click', printContent)
}

initFunction(initPrint)
</script>

</head>

  <body>
    <!-- Include the ThebeLab config so it gets reloaded on each page -->
    <script type="text/x-thebe-config">{
    requestKernel: true,
    binderOptions: {
    repo: "rtavenar/hdr",
    ref: "gh-pages",
    },
    codeMirrorConfig: {
    theme: "abcdef",
    mode: "python"
    },
    kernelOptions: {
    kernelName: "python3",
    path: ""
    }
}
</script>

    <!-- .js-show-sidebar shows sidebar by default -->
    <div id="js-textbook" class="c-textbook js-show-sidebar">
      



<nav id="js-sidebar" class="c-textbook__sidebar">
  
  <h2 class="c-sidebar__title">Machine Learning for Time Series</h2>
  <ul class="c-sidebar__chapters">
    
      
      

      
      
      
      

      
      
      <li class="c-sidebar__chapter" data-url="/parts/about">
        <a class="c-sidebar__entry"
          href="/hdr/parts/about.html"
        >
          
          About this document
        </a>
      </li>

      
      

      

      
      

      

      
    
      
      

      
      
      
      

      
      
      <li class="c-sidebar__chapter" data-url="/parts/intro">
        <a class="c-sidebar__entry"
          href="/hdr/parts/intro.html"
        >
          
          Introduction
        </a>
      </li>

      
      

      

      
      

      

      
    
      
      

      
      
      
      

      
      
      <li class="c-sidebar__chapter" data-url="/parts/01_metrics">
        <a class="c-sidebar__entry"
          href="/hdr/parts/01_metrics.html"
        >
          
            1.
          
          Defining adequate metrics for structured data
        </a>
      </li>

      
      

      

      
      

      
        

        

        <ul class="c-sidebar__sections">
          
            
            

            
            
            
            

            <li class="c-sidebar__section" data-url="/parts/01/temporal_kernel">
              <a class="c-sidebar__entry"
                href="/hdr/parts/01/temporal_kernel.html"
              >
                
                  1.1
                
                A Temporal Kernel for Time Series
              </a>
            </li>
            
            
          
            
            

            
            
            
            

            <li class="c-sidebar__section" data-url="/parts/01/dtw">
              <a class="c-sidebar__entry"
                href="/hdr/parts/01/dtw.html"
              >
                
                  1.2
                
                Dynamic Time Warping
              </a>
            </li>
            
              
              <ul class='c-sidebar__subsections'>
              
                
                
                
                
                <li class="c-sidebar__subsection" data-url="/parts/01/dtw/dtw_warping_length">
                  <a class="c-sidebar__entry"
                    href="/hdr/parts/01/dtw/dtw_warping_length.html"
                  >
                    
                      1.2.1
                      
                    
                    Constrained Dynamic Time Warping
                  </a>
                </li>
              
                
                
                
                
                <li class="c-sidebar__subsection" data-url="/parts/01/dtw/dtw_da">
                  <a class="c-sidebar__entry"
                    href="/hdr/parts/01/dtw/dtw_da.html"
                  >
                    
                      1.2.2
                      
                    
                    DTW alignment as an adaptive resampling strategy
                  </a>
                </li>
              
                
                
                
                
                <li class="c-sidebar__subsection" data-url="/parts/01/dtw/dtw_gi">
                  <a class="c-sidebar__entry"
                    href="/hdr/parts/01/dtw/dtw_gi.html"
                  >
                    
                      1.2.3
                      
                    
                    DTW with Global Invariances
                  </a>
                </li>
              
              </ul>
            
            
          
            
            

            
            
            
            

            <li class="c-sidebar__section" data-url="/parts/01/ot">
              <a class="c-sidebar__entry"
                href="/hdr/parts/01/ot.html"
              >
                
                  1.3
                
                Optimal Transport for Structured Data
              </a>
            </li>
            
            
          
        </ul>
      

      
    
      
      

      
      
      
      

      
      
      <li class="c-sidebar__chapter" data-url="/parts/02_representations">
        <a class="c-sidebar__entry"
          href="/hdr/parts/02_representations.html"
        >
          
            2.
          
          Learning sensible representations for time series
        </a>
      </li>

      
      

      

      
      

      
        

        

        <ul class="c-sidebar__sections">
          
            
            

            
            
            
            

            <li class="c-sidebar__section" data-url="/parts/02/topic_models">
              <a class="c-sidebar__entry"
                href="/hdr/parts/02/topic_models.html"
              >
                
                  2.1
                
                Temporal Topic Models
              </a>
            </li>
            
            
          
            
            

            
            
            
            

            <li class="c-sidebar__section" data-url="/parts/02/shapelets_cnn">
              <a class="c-sidebar__entry"
                href="/hdr/parts/02/shapelets_cnn.html"
              >
                
                  2.2
                
                Shapelet-based Representations and Convolutional Models
              </a>
            </li>
            
            
          
            
            

            
            
            
            

            <li class="c-sidebar__section" data-url="/parts/02/early">
              <a class="c-sidebar__entry"
                href="/hdr/parts/02/early.html"
              >
                
                  2.3
                
                Early Classification of Time Series
              </a>
            </li>
            
            
          
        </ul>
      

      
    
      
      

      
      
      
      

      
      
      <li class="c-sidebar__chapter" data-url="/parts/conclu">
        <a class="c-sidebar__entry"
          href="/hdr/parts/conclu.html"
        >
          
          Conclusion and Perspectives
        </a>
      </li>

      
      

      

      
      

      

      
    
      
      
        <li class="c-sidebar__divider"></li>
        
      
      

      
      
      
      

      
      
      <li class="c-sidebar__chapter" data-url="/contributing">
        <a class="c-sidebar__entry"
          href="/hdr/contributing.html"
        >
          
          Feedback welcome
        </a>
      </li>

      
      

      

      
      

      

      
    
  </ul>
  <p class="sidebar_footer">Romain Tavenard's HDR thesis.<br />Powered by <a href='https://jupyterbook.org'>Jupyter Book</a>.</p>
</nav>

      
      <div class="c-topbar" id="top-navbar">
  <!-- We show the sidebar by default so we use .is-active -->
  <div class="c-topbar__buttons">
    <button
      id="js-sidebar-toggle"
      class="hamburger hamburger--arrowalt is-active"
    >
      <span class="hamburger-box">
        <span class="hamburger-inner"></span>
      </span>
    </button>
    <div class="buttons">
<div class="download-buttons-dropdown">
    <button id="dropdown-button-trigger" class="interact-button"><img src="/hdr/assets/images/download-solid.svg" alt="Download" /></button>
    <div class="download-buttons">
        
        <a id="interact-button-print"><button id="interact-button-download" class="interact-button">.pdf</button></a>
    </div>
</div>


</div>

  </div>
  <!-- Empty sidebar placeholder that we'll auto-fill with javascript -->
  <aside class="sidebar__right">
    <header><h4 class="nav__title"><img src="/hdr/assets/images/list-solid.svg" alt="Search" />   On this page</h4></header>
    <nav class="onthispage">
    </nav>
  </aside>
  <a href="/hdr/search.html" class="topbar-right-button" id="search-button">
    <img src="/hdr/assets/images/search-solid.svg" alt="Search" />
  </a>
</div>

      <main class="c-textbook__page" tabindex="-1">
            <div class="c-textbook__content" id="textbook_content">
              <h1 id="shapelet-based-representations-and-convolutional-models">Shapelet-based Representations and Convolutional Models</h1>

<p>In this section, we will cover works that either relate to the Shapelet
representation for time series or to the family of (1d) Convolutional Neural
Networks, since these two families of methods are very similar in spirit
<a class="citation" href="#lods:hal-01565207">(Lods, Malinowski, Tavenard, &amp; Amsaleg, 2017)</a>.</p>

<h2 id="data-augmentation-for-time-series-classification">Data Augmentation for Time Series Classification</h2>

<!-- #region {"tags": ["popout"]} -->
<p><strong>Note.</strong> This work is part of Arthur Le Guennec’s Master internship.
We were co-supervising Arthur together with Simon Malinowski.
<!-- #endregion --></p>

<p>We have shown in <a class="citation" href="#leguennec:halshs-01357973">(Le Guennec, Malinowski, &amp; Tavenard, 2016)</a> that augmenting time
series classification datasets was an efficient way to improve generalization
for shallow Convolutional Neural Networks.
The data augmentation strategies that were investigated in this work are
local warping and window slicing and both lead to improvements.</p>

<h2 id="learning-to-mimic-a-target-distance">Learning to Mimic a Target Distance</h2>

<!-- #region {"tags": ["popout"]} -->
<p><strong>Note.</strong> This work is part of Arnaud Lods’ Master internship.
We were co-supervising Arnaud together with Simon Malinowski.
<!-- #endregion --></p>

<p>Another track of research we have lead concerns unsupervised representation
learning for time series.
In this context, our approach has consisted in learning a representation in
order to mimic a target distance.</p>

<p>As presented in <a href="../01/dtw.html">Sec. 1.2</a>, Dynamic Time Warping is a widely
used similarity measure for time series.
However, it suffers from its non differentiability and the fact that it does
not satisfy metric properties.
Our goal in <a class="citation" href="#lods:hal-01565207">(Lods, Malinowski, Tavenard, &amp; Amsaleg, 2017)</a> was to introduce a Shapelet model that
extracts latent representations such that Euclidean distance in the latent
space is as close as possible to Dynamic Time Warping between original time
series.
The resulting model is an instance of a Siamese Network:</p>

<p><img src="../../images/siamese_ldps.png" alt="" /></p>

<p>where $m_\theta(\cdot)$ is the feature extraction part of the model that
maps a time series to its shapelet transform representation.</p>

<p>```python  tags=[“hide_input”]
%config InlineBackend.figure_format = ‘svg’
import matplotlib.pyplot as plt
import torch.nn as nn
import torch
import numpy
from torch.autograd import Variable
from sklearn.linear_model import LinearRegression
from tslearn.clustering import TimeSeriesKMeans
from tslearn.metrics import dtw</p>

<p>plt.ion()</p>

<p>def <em>kmeans_init_shapelets(X, n_shapelets, shp_len, n_draw=10000):
    n_ts, sz, d = X.shape
    indices_ts = numpy.random.choice(n_ts, size=n_draw, replace=True)
    indices_time = numpy.random.choice(sz - shp_len + 1,
                                       size=n_draw,
                                       replace=True)
    subseries = numpy.zeros((n_draw, shp_len, d))
    for i in range(n_draw):
        subseries[i] = X[indices_ts[i],
                         indices_time[i]:indices_time[i] + shp_len]
    return TimeSeriesKMeans(n_clusters=n_shapelets,
                            metric=”euclidean”,
                            verbose=False).fit(subseries).cluster_centers</em></p>

<p>def tslearn2torch(X):
    X_ = torch.Tensor(numpy.transpose(X, (0, 2, 1)))
    return X_</p>

<p>class MinPool1d(nn.Module):
    “”” Simple Hack for 1D min pooling. Input size = (N, C, L_in)
        Output size = (N, C, L_out) where N = Batch Size, C = No. Channels
        L_in = size of 1D channel, L_out = output size after pooling.
        This implementation does not support custom strides, padding or dilation
        Input shape compatibilty by kernel_size needs to be ensured</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    This code comes from:
    https://github.com/reachtarunhere/pytorch-snippets/blob/master/min_pool1d.py
    (under MIT license)
    """

def __init__(self, kernel_size=3):
    super(MinPool1d, self).__init__()
    self.kernel_size = kernel_size

def forward(self, l):
    N, C, L = [l.size(i) for i in range(3)]
    l = l.view(N, C, int(L / self.kernel_size), self.kernel_size)
    return l.min(dim=3)[0].view(N, C, -1)
</code></pre></div></div>

<p>class ShapeletLayer(nn.Module):
    “"”Shapelet layer.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Computes sliding window distances between a set of time series and a set
of shapelets.

Parameters
----------
in_channels : int
    Number of input channels (modalities of the time series)
out_channels: int
    Number of output channels (number of shapelets)
kernel_size: int
    Shapelet length

Examples
--------
&gt;&gt;&gt; time_series = torch.Tensor([[1., 2., 3., 4., 5.], [-4., -5., -6., -7., -8.]]).view(2, 1, 5)
&gt;&gt;&gt; shapelets = torch.Tensor([[1., 2.], [3., 4.], [5., 6.]])
&gt;&gt;&gt; layer = ShapeletLayer(in_channels=1, out_channels=3, kernel_size=2)
&gt;&gt;&gt; layer.weight.data = shapelets
&gt;&gt;&gt; dists = layer.forward(time_series)
&gt;&gt;&gt; dists.shape
torch.Size([2, 3, 4])
&gt;&gt;&gt; dists[0]
tensor([[ 0.,  1.,  4.,  9.],
        [ 4.,  1.,  0.,  1.],
        [16.,  9.,  4.,  1.]], grad_fn=&lt;SelectBackward&gt;)
"""
def __init__(self, in_channels, out_channels, kernel_size):
    super(ShapeletLayer, self).__init__()

    self.kernel_size = kernel_size
    self.out_channels = out_channels

    self.false_conv_layer = nn.Conv1d(in_channels=in_channels,
                                      out_channels=in_channels,
                                      kernel_size=kernel_size,
                                      bias=False)
    data = torch.Tensor(numpy.eye(kernel_size))
    self.false_conv_layer.weight.data = data.view(kernel_size,
                                                  1,
                                                  kernel_size)
    for p in self.false_conv_layer.parameters():
        p.requires_grad = False
    self.weight = nn.Parameter(torch.Tensor(out_channels,
                                            kernel_size))

def forward(self, x):
    reshaped_x = self.false_conv_layer(x)
    reshaped_x = torch.transpose(reshaped_x, 1, 2)
    reshaped_x = reshaped_x.contiguous().view(-1, self.kernel_size)
    distances = self.pairwise_distances(reshaped_x, self.weight)
    distances = distances.view(x.size(0), -1, self.out_channels)
    return torch.transpose(distances, 1, 2)

@classmethod
def pairwise_distances(cls, x, y):
    """Computes pairwise distances between vectors in x and those in y.

    Computed distances are normalized (i.e. divided) by the dimension of
    the space in which vectors lie.
    Assumes x is 2d (n, d) and y is 2d (l, d) and returns
    a tensor of shape (n, l).

    Parameters
    ----------
    x : Tensor of shape=(n, d)
    y : Tensor of shape=(l, d)

    Returns
    -------
        A 2d Tensor of shape (n, l)
    """
    L = y.size(-1)
    x_norm = (x ** 2).sum(1).view(-1, 1)
    y_t = torch.transpose(y, 0, 1)
    y_norm = (y ** 2).sum(1).view(1, -1)

    dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)
    return torch.clamp(dist, 0.0, numpy.inf) / L ```
</code></pre></div></div>

<p>Here is a <code class="highlighter-rouge">PyTorch</code> implementation of this model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LDPSModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""Learning DTW-Preserving Shapelets (LDPS) model.

    Parameters
    ----------
    n_shapelets_per_size: dict (optional, default: None)
        Dictionary giving, for each shapelet size (key),
        the number of such shapelets to be trained (value)
        None should be used only if `load_from_disk` is set
    ts_dim: int (optional, default: None)
        Dimensionality (number of modalities) of the time series considered
        None should be used only if `load_from_disk` is set
    lr: float (optional, default: 0.01)
        Learning rate
    epochs: int (optional, default: 500)
        Number of training epochs
    batch_size: int (optional, default: 64)
        Batch size for training procedure
    verbose: boolean (optional, default: True)
        Should verbose mode be activated

    Note
    ----
        This implementation requires a dataset of equal-sized time series.
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_shapelets_per_size</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>  <span class="c1"># dict sz_shp -&gt; n_shp
</span>                 <span class="n">ts_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">lr</span><span class="o">=</span><span class="mf">.01</span><span class="p">,</span>
                 <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LDPSModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_shapelets_per_size</span> <span class="o">=</span> <span class="n">n_shapelets_per_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ts_dim</span> <span class="o">=</span> <span class="n">ts_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_set_layers_and_optim</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_set_layers_and_optim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shapelet_sizes</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_shapelets_per_size</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shapelet_blocks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_shapelet_blocks</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_shapelet_blocks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">ShapeletLayer</span><span class="p">(</span>
                <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ts_dim</span><span class="p">,</span>
                <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_shapelets_per_size</span><span class="p">[</span><span class="n">shapelet_size</span><span class="p">],</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="n">shapelet_size</span>
            <span class="p">)</span> <span class="k">for</span> <span class="n">shapelet_size</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">shapelet_sizes</span>
        <span class="p">])</span>

    <span class="k">def</span> <span class="nf">_temporal_pooling</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">pool_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pooled_x</span> <span class="o">=</span> <span class="n">MinPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pooled_x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">pooled_x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">features_maxpooled</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">shp_sz</span><span class="p">,</span> <span class="n">block</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shapelet_sizes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shapelet_blocks</span><span class="p">):</span>
            <span class="n">f</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">f_maxpooled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temporal_pooling</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
            <span class="n">features_maxpooled</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f_maxpooled</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">features_maxpooled</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_init_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">shapelet_blocks</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shapelet_initializer</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

        <span class="c1"># Initialize scaling using linear regression
</span>        <span class="n">pair</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_batch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_fit_</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scaling_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>  <span class="c1"># Start without scale
</span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>
        <span class="n">reg_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">reg_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">targets</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scaling_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">reg_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">_shapelet_initializer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
        <span class="n">X_npy</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_fit_</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">shapelets_npy</span> <span class="o">=</span> <span class="n">_kmeans_init_shapelets</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_npy</span><span class="p">,</span>
                                               <span class="n">n_shapelets</span><span class="o">=</span><span class="n">w</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                                               <span class="n">shp_len</span><span class="o">=</span><span class="n">w</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">w</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">shapelets_npy</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">w</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">w</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">xi</span><span class="p">,</span> <span class="n">xj</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">emb_xi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_features</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span>
        <span class="n">emb_xj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_features</span><span class="p">(</span><span class="n">xj</span><span class="p">)</span>
        <span class="n">norm_ij</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">emb_xi</span> <span class="o">-</span> <span class="n">emb_xj</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">scaled_norm_ij</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling_layer</span><span class="p">(</span><span class="n">norm_ij</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scaled_norm_ij</span>

    <span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">batch_indices1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">batch_indices2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">X1</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">batch_indices1</span><span class="p">]</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                      <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">X2</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">batch_indices2</span><span class="p">]</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                      <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">targets_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="n">dtw</span><span class="p">(</span><span class="n">X1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
                                       <span class="c1"># NOTE: tslearn dim ordering is reverse
</span>                                       <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)])</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">targets_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">),</span> <span class="n">targets</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="s">"""Learn shapelets and weights for a given dataset.

        Parameters
        ----------
        X : numpy array of shape=(n_ts, sz, 1)
            Time series dataset

        Returns
        -------
        LDPSModel
            The fitted model
        """</span>
        <span class="n">X_</span> <span class="o">=</span> <span class="n">tslearn2torch</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_fit_</span> <span class="o">=</span> <span class="n">X_</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">()</span>
        <span class="n">loss_fun</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

        <span class="n">n_batch_per_epoch</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">X_</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
            <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_batch_per_epoch</span><span class="p">):</span>
                <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_batch</span><span class="p">(</span><span class="n">X_</span><span class="p">)</span>

                <span class="c1"># zero the parameter gradients
</span>                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="c1"># forward + backward + optimize
</span>                <span class="n">preds</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fun</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

                <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="ow">and</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">'[</span><span class="si">%</span><span class="s">d] loss: </span><span class="si">%.3</span><span class="s">f'</span> <span class="o">%</span>
                          <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">running_loss</span> <span class="o">/</span> <span class="n">n_batch_per_epoch</span><span class="p">)</span>
                         <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></div>

<p>and a simple example usage:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tslearn.datasets</span> <span class="kn">import</span> <span class="n">UCR_UEA_datasets</span>
<span class="n">X</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">UCR_UEA_datasets</span><span class="p">()</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s">"SwedishLeaf"</span><span class="p">)</span>

<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LDPSModel</span><span class="p">(</span><span class="n">n_shapelets_per_size</span><span class="o">=</span><span class="p">{</span><span class="mi">30</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
                  <span class="n">lr</span><span class="o">=</span><span class="mf">.001</span><span class="p">,</span>
                  <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                  <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>

<span class="n">pairs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_batch</span><span class="p">(</span><span class="n">tslearn2torch</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">10</span><span class="p">]))</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
            <span class="n">targets</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
            <span class="n">color</span><span class="o">=</span><span class="s">"b"</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">"x"</span><span class="p">)</span>
<span class="n">max_val</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="nb">max</span><span class="p">(),</span> <span class="n">targets</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="n">max_val</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="n">max_val</span><span class="p">],</span> <span class="s">"r-"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Distance in the Shapelet Transform space"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Dynamic Time Warping distance"</span><span class="p">);</span>
</code></pre></div></div>

<p>We have shown that such a model could be used as a feature extractor on top of
which a $k$-means clustering could operate efficiently.
We have also shown in <a class="citation" href="#carlinisperandio:hal-01841995">(Carlini Sperandio, Malinowski, Amsaleg, &amp; Tavenard, 2018)</a> that this
representation is useful for time series indexing tasks.</p>

<h2 id="including-localization-information">Including Localization Information</h2>

<!-- #region {"tags": ["popout"]} -->
<p><strong>Note.</strong> This work was part of Mael Guillemé’s PhD thesis.
I was not involved in Mael’s PhD supervision.
<!-- #endregion --></p>

<p>The shapelet transform, as defined above, does not hold localization
information. Several options could be considered to add such kind of
information. First, the global pooling step could be turned into local pooling
to keep track of local shapelet distances.
In (missing reference)}, we rather focused on augmenting the feature
representation with shapelet match localization features.</p>

<p>Relying on a set of random shapelets (shapelets that are extracted uniformly at
random from the set of all subseries in the training set)
${\mathbf{s_k}}<em>{k &lt; p}$,
each time series is embedded into a $2p$-dimensional feature that stores, for
each shapelet, the shapelet distance $d</em>{\mathbf{s_k}}(\cdot)$ as well as
optimal  match localization $l_{\mathbf{s_k}}(\cdot)$:</p>

<p>\begin{eqnarray}
    d_{\mathbf{s_k}}(\mathbf{x}) &amp;=&amp; \min_t
        |\mathbf{x}<em>{t \rightarrow t+L_k} - \mathbf{s_k}| <br />
    l</em>{\mathbf{s_k}}(\mathbf{x}) &amp;=&amp; \arg \min_t
        |\mathbf{x}_{t \rightarrow t+L_k} - \mathbf{s_k}|
\end{eqnarray}</p>

<p>where $L_k$ is the length of the $k$-th shapelet and
$\mathbf{x}_{t \rightarrow t+L_k}$
is the subseries from $\mathbf{x}$ that starts at timestamp $t$ and has length
$L_k$.</p>

<p>In the random shapelet setting, a large number of shapelets are drawn and
feature selection is used afterwards to focus on most useful shapelets.
In our specific context, we have introduced a structured feature selection
mechanism that allows, for each shapelet, to either:</p>

<ul>
  <li>discard all information (match magnitude and localization);</li>
  <li>keep shapelet distance information and discard localization information;</li>
  <li>keep all information (match magnitude and localization).</li>
</ul>

<p>To do so, we have introduced a modified Group-Lasso (called
Semi-Sparse Group Lasso) loss that allows to enforce sparsity on some individual
variables only:</p>

<p>\begin{equation}
    \mathcal{L}^{\mathrm{SSGL}}(y, \hat{y}, \boldsymbol{\theta}) =
        \mathcal{L}(y, \hat{y}, \boldsymbol{\theta})
        + \alpha \lambda
            \left| \mathbf{M}<em>\text{ind} \boldsymbol{\beta} \right|_1
        + (1-\alpha) \lambda \sum</em>{k=1}^{K} \sqrt{p_k}
            \left| \boldsymbol{\beta}^{(k)} \right|_2
\end{equation}</p>

<p>where $\mathbf{M}<em>\text{ind}$ is a diagonal indicator matrix that has ones on
the diagonal for features that could be discarded individually (localization
features in our random shapelet case), $\boldsymbol{\theta}$ is the set of
all model weights, including weights $\boldsymbol{\beta}$ that are directly
connected to the features (_ie.</em> these are weights from the first layer), that
are organized in groups $\boldsymbol{\beta}^{(k)}$ of size $p_k$ ($p_k=2$ in the
random shapelet context), each group corresponding to a different shapelet.</p>

<p>```python tags=[“hide_input”]
%config InlineBackend.figure_format = ‘svg’
import matplotlib.pyplot as plt
import numpy</p>

<p>plt.ion()</p>

<p>def plot_results(beta_star,
                 beta_hat_ssgl=None, mse_ssgl=None,
                 beta_hat_sgl=None, mse_sgl=None):
    plt.figure(figsize=(6, 2))
    plt.set_cmap(“tab10”)
    ind = numpy.arange(dim)
    width = 0.25
    plt.bar(ind, numpy.abs(beta_star), width,
            label=”Ground-truth”,
            lw=2,
            alpha=0.6)
    if beta_hat_ssgl is not None:
        plt.bar(ind+width, numpy.abs(beta_hat_ssgl), width,
                label=”SSGL (MSE=%.2f)” % mse_ssgl,
                lw=2,
                alpha=0.6)
    if beta_hat_sgl is not None:
        plt.bar(ind+2*width, numpy.abs(beta_hat_sgl), width,
                label=”SGL (MSE=%.2f)” % mse_sgl,
                lw=2,
                alpha=0.6)
    plt.gca().set_yscale(“log”)</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>labels = []
for group_id in range(3):
    for feature_id in range(n_features_per_group):
        labels.append("$\\beta_%d^{(%d)}$" % (feature_id + 1, group_id + 1))
plt.legend(loc="upper left")
plt.ylabel("Beta values")
plt.gca().set_xticklabels(labels)
plt.gca().set_xticks(numpy.arange(6) + .3)
plt.tight_layout(pad=2.) ```
</code></pre></div></div>

<p>Given data generated with a process that is similar to what our SSGL
regularization can handle:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">n_groups</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_features_per_group</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">noise_level</span> <span class="o">=</span> <span class="mf">.01</span>

<span class="n">dim</span> <span class="o">=</span> <span class="n">n_groups</span> <span class="o">*</span> <span class="n">n_features_per_group</span>
<span class="n">groups</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_groups</span><span class="p">),</span> <span class="n">n_features_per_group</span><span class="p">)</span>
<span class="n">ind_sparse</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">dim</span><span class="p">,))</span>
<span class="n">ind_sparse</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">ind_sparse</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">ind_sparse</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>

<span class="n">beta_star</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_star</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise_level</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta_star</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise_level</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</code></pre></div></div>

<p>and using the following <code class="highlighter-rouge">PyTorch</code> code for our model definition:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="k">class</span> <span class="nc">SemiSparseGroupLasso</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_input</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">indices_sparse</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">lbda</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim_input</span> <span class="o">=</span> <span class="n">dim_input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_input</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">groups</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indices_sparse</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">indices_sparse</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lbda</span> <span class="o">=</span> <span class="n">lbda</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_groups</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">groups</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">coef_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">loss_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">l1_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices_sparse</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">l</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lbda</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_norm</span>
        <span class="k">for</span> <span class="n">gr_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_groups</span><span class="p">:</span>
            <span class="n">indices_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">==</span> <span class="n">gr_id</span>
            <span class="n">p_k</span> <span class="o">=</span> <span class="n">indices_k</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>
            <span class="n">l2_norm_k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices_k</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">l</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lbda</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_k</span><span class="p">)</span> <span class="o">*</span> <span class="n">l2_norm_k</span>
        <span class="k">return</span> <span class="n">l</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">X_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="k">assert</span> <span class="n">X_</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_input</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">X_</span><span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">predict_numpy1d</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">X_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="n">X_</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">))</span>
</code></pre></div></div>

<p>we get:</p>

<p>```python tags=[“hide_input”]
from sklearn.linear_model import Lasso</p>

<p>numpy.random.seed(0)
torch.manual_seed(0)</p>

<p>model_ssgl = SemiSparseGroupLasso(dim_input=6, groups=groups,
                                  indices_sparse=ind_sparse,
                                  alpha=.5, lbda=1e-2)
model_ssgl.fit(X, y)
beta_hat_ssgl = model_ssgl.coef_.flatten()
mse_ssgl = numpy.sum((model_ssgl.predict_numpy1d(X_test) - y_test) ** 2)</p>

<p>model_sgl = SemiSparseGroupLasso(dim_input=6, groups=groups,
                                 indices_sparse=numpy.ones(ind_sparse.shape),
                                 alpha=.5, lbda=1e-2)
model_sgl.fit(X, y)
beta_hat_sgl = model_sgl.coef_.flatten()
mse_sgl = numpy.sum((model_sgl.predict_numpy1d(X_test) - y_test) ** 2)</p>

<p>plot_results(
    beta_star,
    beta_hat_ssgl=beta_hat_ssgl, mse_ssgl=mse_ssgl,
    beta_hat_sgl=beta_hat_sgl, mse_sgl=mse_sgl
)
```</p>

<p>and one can notice that SSGL slightly outperforms Sparse-Group Lasso (SGL) in
terms of both Mean Squared Error (MSE) and estimation of zero coefficients.</p>

<p>When applied to the specific case of random shapelets, we have shown that this
lead to improved accuracy as soon as datasets are large enough for coefficients
to be properly estimated.</p>

<h2 id="learning-shapelets-that-look-like-time-series-snippets">Learning Shapelets that Look Like Time Series Snippets</h2>

<p><strong>TODO: decide whether I should include this or not</strong></p>

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="lods:hal-01565207">Lods, A., Malinowski, S., Tavenard, R., &amp; Amsaleg, L. (2017). Learning DTW-Preserving Shapelets. In <i>IDA 2017 - 16th International Symposium on Intelligent Data Analysis</i> (Vol. 10584, pp. 198–209). London, United Kingdom: springer International Publishing.</span></li>
<li><span id="leguennec:halshs-01357973">Le Guennec, A., Malinowski, S., &amp; Tavenard, R. (2016). Data Augmentation for Time Series Classification using Convolutional Neural Networks. In <i>ECML/PKDD Workshop on Advanced Analytics and Learning on Temporal Data</i>. Riva Del Garda, Italy.</span></li>
<li><span id="carlinisperandio:hal-01841995">Carlini Sperandio, R., Malinowski, S., Amsaleg, L., &amp; Tavenard, R. (2018). Time Series Retrieval using DTW-Preserving Shapelets. In <i>SISAP 2018 – 11th International Conference on Similarity Search and Applications</i> (pp. 257–270). Lima, Peru: Springer.</span></li></ol>

            </div>
            <div class="c-textbook__footer" id="textbook_footer">
              
<nav class="c-page__nav">
  

  
</nav>

              <footer>
  <p class="footer"></p>
</footer>

            </div>

        </div>
      </main>
    </div>
  </body>
</html>
